# Baby Tetris MDP

This project implements a Markov Decision Process (MDP) model of a simplified “Baby Tetris” game (4 columns, limited height) and solves it using value iteration. It also includes scripts to run experiments and print all reachable states and actions.

## Requirements

- Standard Linux environment
- POSIX shell (`/bin/sh` or `bash`)
- A C++ compiler with C++17 support (e.g. `g++`)
- No external libraries are required

## Build Instructions

From the project root directory, run:

```bash
./build.sh
```

This script will:

* Create a `build/` directory (if it does not exist)
* Compile the project into two executables inside `build/`:

  * `run_experiments`
  * `print_all_states_and_actions`

If the script is not executable, make it executable first:

```bash
chmod +x build.sh
./build.sh
```

## Running the Programs

After a successful build, go to the `build/` directory:

```bash
cd build
```

### 1. Run experiments

```bash
./run_experiments
```

This executable:

* Runs value iteration for several discount factors `γ`
* Prints the number of iterations, estimated value of the initial state, approximate gain, and execution time
* Simulates episodes to report average total score, average episode length, and average score per move for the optimal policy (and possibly baselines)

### 2. Print all states and actions

```bash
./print_all_states_and_actions
```

This executable:

* Enumerates all reachable MDP states
* Prints each state, the current piece, and the list of available actions
* Shows the best action according to the computed optimal value function


# Assignment Overview

The project must be done **in pairs** (with at most **one group of three**).
General discussion between groups is allowed, but **all code and reports must be your own work**.

Use of **generative AI tools must be kept to a minimum**.
In particular, **the code must *not* be generated by AI**.
Both reports must clearly state if generative AI was used at all, and for what.

You must submit **two reports**:

* **Report 1** — answers **Question 1 only**.
* **Report 2** — answers **all questions (Q1–Q3)**.
  You do not need to repeat the full content of Q1 in Report 2, but you may improve/extend your Q1 solution there.

At the end of the semester, each group will give a **20-minute presentation** of the project (including questions).

### Important Dates

* **Deadline for Report 1 (Question 1 only):** November 30, 2025
* **Deadline for Report 2 (all questions):** January 8, 2026
* **Project Presentations:** January 12, 2026

---

## Baby Tetris: Game Description

We study a simplified Tetris variant called **Baby Tetris**:

* The board has **4 columns** and a bounded height (in Q1: max height 4; the game ends if the stack reaches height 5).
* There are only **two types of falling pieces**:

  1. Three squares in a straight line,
  2. Three squares forming a right angle (L-shaped piece).

### Scoring Rules

When a single piece is dropped and lands:

* If it creates **1 full line** → you score **1 point**, and that line is removed.
* If it creates **2 full lines at once** → you score **3 points**, and both lines are removed.
* If it creates **3 full lines at once** → you score **6 points**, and all three lines are removed.

---

## Questions

### Question 1 — MDP for Baby Tetris (4×4)

We consider a Tetris board with:

* **4 columns**,
* **maximum height 4** (the game ends when the stack reaches height 5).

You must:

1. **Construct an MDP** that models this game:

   * state space (e.g., stack configuration, current piece, etc.),
   * action space (placements and rotations of pieces),
   * transition probabilities,
   * reward function (according to the scoring rules),
   * discount factor and optimality criterion: **maximize the expected sum of discounted rewards** until the game ends (or indefinitely, if it can in principle last forever).
2. **Implement a program** that:

   * builds this MDP (states, actions, transitions, rewards),
   * computes an **optimal policy** that maximizes the expected discounted total score.
3. As a challenge, **extend the game** to:

   * height 5 with 4 columns, and/or
   * an even larger Baby Tetris.

**Report 1** must focus on:

* the MDP formulation,
* the implementation details,
* numerical results (value function, example policies, comparisons, etc.).

---

### Question 2 — Adversarial Piece Selection (Stochastic Game)

Now the falling pieces are chosen by an **adversary** whose goal is to **minimize your score**.

You must:

1. Show that this setting defines a **stochastic (zero-sum) game**:

   * players: you (choosing placements) vs. the adversary (choosing pieces),
   * opposite objectives: you maximize, the adversary minimizes the expected reward.
2. **Compute optimal strategies**:

   * the optimal strategy for the player,
   * the optimal strategy for the adversary,
   * the value of the game under optimal play.

In **Report 2**, you should:

* formulate the stochastic game precisely,
* explain the solution method (e.g., dynamic programming for games, value/policy iteration for zero-sum stochastic games),
* present and interpret your numerical results.

---

### Question 3 — Unknown Adversary

Now the player does **not know** how the adversary chooses pieces. The adversary may:

* play the optimal minimization strategy from Q2,
* choose pieces randomly,
* use any other strategy, possibly **non-stationary**.

You must:

1. Propose ideas for a **player strategy** under this uncertainty:

   * robust strategies,
   * adaptive / learning-based strategies (e.g., online learning, exploration–exploitation),
   * or combinations of such approaches.
2. Propose a way to **evaluate the efficiency** of the player’s strategy:

   * against different types of adversaries (optimal, random, “worst-case”, etc.),
   * compared to the optimal strategies from Q1/Q2,
   * using suitable metrics (average score, regret, etc.).

In **Report 2**, you should:

* justify your choice of player strategy,
* describe how it is computed or learned,
* present experimental results and comparisons.

---
